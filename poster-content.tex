\hyphenation{al-though Al-though}

\section{Background}
\label{sec:background}

The work presented here focuses on creating analytics services that can be automatically created and hosted on any of the clouds supported by cloudmesh. This is a non-trivial effort due to the large number of technologies involved and is outside of the expertise of domain scientists. However, the use of cloudmesh makes it possible for the domain scientist to easily access these services and leverage our more than ten years of experience in this field.

\section{Benchmark}
\label{sec:benchmark}

\subsection{Application}
We provide two example benchmarks for the Eigenfaces SVM example. The
first deploys and measures the AI service on a single cloud provider at
a time (see \ref{single-cloud-provider-service-benchmarking}), and the second deploys a multi-cloud (see \ref{sec-multi-benchmark}) AI service measuring the service across the clouds in parallel.

\begin{table}
  
\caption{Controlled VM parameters for cloud benchmarks.}
\label{tab:iaas}

\resizebox{1.0\columnwidth}{!}{
\begin{tabular}[]{@{}llll@{}}
\toprule
 & AWS & Azure & Google \tabularnewline
\midrule
%\endhead
Size (flavor) & m4.large & Standard\_D2s\_v3 & n1-standard-2 \tabularnewline
vCPU & 2 & 2 & 2 \tabularnewline
Memory (GB) & 8 & 8 & 7.5 \tabularnewline
Image & ami-0dba2cb6798deb6d8
& \begin{minipage}[t]{0.40\columnwidth}\raggedright
Canonical:0001-com-ubuntu-server-focal:20\_04-lts:20.04.202006100\strut
\end{minipage} & 
ubuntu-2004-lts
\tabularnewline
OS & Ubuntu 20.04 LTS & Ubuntu 20.04 LTS & Ubuntu 20.04 LTS \tabularnewline
Region & us-east-1 & eastus & us-east1 \tabularnewline
Zone &  N/A &  N/A & us-east1-b \tabularnewline
Price (\$/hr) &  0.1 & 0.096 & 0.0949995 \tabularnewline
%Runs/Test & 90 & 60 & 90\tabularnewline
\bottomrule
\end{tabular}
}
\bigskip

\caption{Raspberry Pi and Docker Specifications}
\label{tab:pi}
\resizebox{1.0\columnwidth}{!}{
\begin{tabular}[]{lllll}
\toprule 
&        &      &  Docker     & MacBook \tabularnewline
& Pi 3B+ & Pi 4 & (On MBP) & Pro i5 3.1GHz\tabularnewline
\midrule
%\endhead
Cores & 4 & 4 & 2&2\tabularnewline
Memory (GB) & 1 & 8  & 2&8\tabularnewline
OS & Raspberry OS 10 & Raspberry OS 10 & Ubuntu 20.04 LTS & macOS \tabularnewline
Version & Kernel 5.4.51  & Kernel 5.4.51 & NA& Big Sur \tabularnewline
Purchase Cost (\$) & 51.99 & 109.99 & NA& NA\tabularnewline
Energy Cost (\$/year) & 5.36 & 6.73 & NA & NA\tabularnewline
Price (\$/hr) &  0.0065 & 0.0133 & NA & NA\tabularnewline
%Runs/Test & 30 &  30 & 1 &\tabularnewline
\bottomrule
\end{tabular}
}


\smallskip 
{\footnotesize 
The Price is the purchase cost and 1yr energy cost, amortized over a year and given for each hour of the year.}
\end{table}

\subsection{Single Cloud Provider AI Service Benchmark.}
\label{single-cloud-provider-service-benchmarking}

The benchmark runs the pytest in two configurations. After the benchmark
the script sets up a virtual machine environment, it runs the first pytest
locally on the OpenAPI server and measures five runtimes:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item \textbf{download data:} Download and extraction of remote image data from
  ndownloader.figshare.com/files/5976015
\item \textbf{train:} 
  The model training time when run as an OpenAPI service
\item \textbf{scikitlearn train:} 
  The model training time when run as the Scikit-learn example without
  OpenAPI involvement
\item \textbf{upload local:} 
  The time to upload an image from the server to itself
\item \textbf{predict local:} 
  The time to predict and return the target label of the uploaded image
\end{enumerate}

The benchmark runs the second pytest iteration as a remote client and interacts with the deployed OpenAPI service over the internet. It tests two runtimes:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item \textbf{upload remote:} 
  The time to upload an image to the remote OpenAPI server
\item \textbf{predict remote:} 
  The time to run the predict function on the remote OpenAPI server, and
  return the target label of the uploaded image
\end{enumerate}

In \Figure{fig:download} we compare the download and extraction time of the labeled faces in the wild data set. This data set is approximately 233 MBs
compressed, which allows us to measure a non-trivial data transfer.
Lower transfer times imply the cloud has higher throughput from the data
server, less latency to the data server, or that the cloud has a better performing internal network. The standard deviation is displayed to compare the variation in the download times. Because the difference between commercial and residential internet speeds dominates the function runtime, we do not compare the clouds to the Pi models, MacBook, or docker container.
\begin{comment}
In  \Figure{fig:download} we show the same plot without the Pi and docker results to allow a closer comparison of the three comparable clouds.
\end{comment}

\begin{comment}
\TwoFIGURES
    {sample_graph_1_pi1.pdf}
    {Runtime for downloading the data used in the Eigenfaces SVM benchmark.}
    {fig:download_pi}
    {sample_graph_1.pdf}
    {Closeup of the Runtime for downloading the data used in the Eigenfaces SVM benchmark while excluding the runtime for the Raspberry PI.}
    {fig:download}
\end{comment}

\OneFIGURE
    {sample_graph_1.pdf}
    {Runtime for downloading the data used in the Eigenfaces SVM benchmark.}
    {fig:download}
    
In \Figure{fig:train_pi} we measure the training time of the Eigenfaces-SVM model both as an OpenAPI service and as the basic Scikit-learn example. This allows us to measure the runtime overhead added by OpenAPI compared to the source example. Here, the two functions are identical except that the
OpenAPI train function makes an additional function call to store the
model to disk. This is necessary to share the model across the train and predict functions. In the figure there are two bars per cloud provider. The blue bars are the training time of the model when hosted as a Cloudmesh OpenAPI function. The orange bars are the training time of the Scikit-learn example code without Cloudmesh OpenAPI involvement. The bars plot the mean runtimes and the error bar reflects the standard deviation of the runtimes. In  \Figure{fig:train} we show the same plot without the Pi models, MacBook, and docker results to allow a closer comparison of the three comparable clouds.

\TwoFIGURES
    {sample_graph_2_pi1.pdf}
    {Runtime for training on the data used in the Eigenfaces SVM benchmark.}
    {fig:train_pi}
    {sample_graph_2.pdf}
    {Closeup of the Runtime for training on  the data used in the Eigenfaces SVM benchmark without the data for the Pi.}
    {fig:train}


In \Figure{fig:upload_pi} we measure the time to upload an image to the server both
from itself and from a remote client. This allows us to compare the
function runtime as experienced by the server, and as experienced by a
remote client. The difference helps determine the network latency
between the benchmark client and the cloud service. In the figure, there are two bars per
cloud provider. The blue bars are the runtime of the upload function as
experienced by the server, and the orange as experienced by the remote
client. The bars plot the mean runtimes and the error bar reflects the
standard deviation of the runtimes. For the Pi models, MacBook, and docker container, we only measure the local function runtime.

%\TwoFIGURES
\OneFIGURE
    {sample_graph_3_pi1.pdf}
    {Runtime for uploading the data used in the Eigenfaces SVM benchmark.}
    {fig:upload_pi}
    %{sample_graph_3.pdf}
    %{Closeup of the Runtime for uploading the data used in the Eigenfaces SVM benchmark without the data for the Pi.}
    %{fig:upload}

In \Figure{fig:predict_pi} we measure the time to call the predict function on the uploaded image. Again we run this once from the local server itself, and a second time from a remote client to determine client and server runtimes. In the figure, there are two bars per cloud provider. The blue bars are the run time of the predict function as experienced by the server, and the orange as experienced by the remote client. The bars plot mean runtimes and the error bar reflects the standard deviation of the runtimes. For the Pi models, MacBook, and docker container, we only measured the local function runtime.

%\TwoFIGURES
\OneFIGURE
    {sample_graph_4_pi1.pdf}
    {Runtime for the prediction used in the Eigenfaces SVM benchmark.}
    {fig:predict_pi}
    %{sample_graph_4.pdf}
    %{Closeup of the Runtime for the prediction used in the Eigenfaces SVM benchmark without the data for the Pi.}
    %{fig:predict}

\Table{tab:2} presents a full listing of test results. For the upload and predict tests, the 'type' column denotes whether the test was run locally (server runtime) or remote (client runtime).

In \Table{tab:cost} we present a cost analysis of the service functions. The analysis uses the price from \Table{tab:iaas} and \Table{tab:pi}. The price for the cloud virtual machines are based on provider advertised costs, while the price for the Pi models are based on the hardware cost and one year of energy cost amortized for one year. This does not include other costs such as cooling, networking, or real estate. For the Pi energy cost we assume a full and constant load. We utilize power consumption benchmarks from \cite{pi-power} and Indiana residential kWH cost from \cite{indiana-energy} to calculate the expected Energy Cost per year. We calculate the cost to run each function and compare the clouds and Raspberry Pi 4 to the Raspberry Pi 3b+. We compare the percent runtime decrease from the Pi 3b+ to the clouds and Raspberry Pi4, and the percent cost increase from the Pi 3b+ to the clouds and Raspberry Pi 4.

\begin{comment}

\begin{table}[h]
%\caption{example table with vertical labels}
example for vertical labels
\centering
\begin{tabular}{clllrrrr}
\toprule
type & \multicolumn{1}{c}{test} & \multicolumn{1}{c}{cloud} & \multicolumn{1}{c}{mean} & \multicolumn{1}{c}{min} & \multicolumn{1}{c}{max} & \multicolumn{1}{c}{std}\\ 
\hline
\multirow{3}{*}{\rotatebox[origin=c]{90}{\parbox[c]{1cm}{\centering local}}} & 
\multirow{3}{*}{\rotatebox[origin=c]{90}{\parbox[c]{1cm}{\centering down\-load}}} 
  & aws &2&3&4&5\\
& & azure & 2&3&4&5\\
& & google &3&3&4&5\\ 
\hline
\multirow{3}{*}{\rotatebox[origin=c]{90}{\parbox[c]{1cm}{\centering local}}} & 
\multirow{3}{*}{\rotatebox[origin=c]{90}{\parbox[c]{1cm}{\centering predict}}} 
  & aws &2&3&4&5\\
& & azure & 2&3&4&5\\
& & google &3&3&4&5\\ 
\hline
\multirow{3}{*}{\rotatebox[origin=c]{90}{\parbox[c]{1cm}{\centering local}}} & 
\multirow{3}{*}{\rotatebox[origin=c]{90}{\parbox[c]{1cm}{\centering remote}}} 
  & aws &2&3&4&5\\
& & azure & 2&3&4&5\\
& & google &3&3&4&5\\ 
\hline
\end{tabular}
\end{table}
\end{comment}


\begin{table}[htb]
  
\caption{Test results for the Eigenfaces SVM single cloud provider benchmark.}
\label{tab:2}

\resizebox{0.9\columnwidth}{!}{
\begin{tabular}{lllrrrr}
\toprule
              test &    type &     cloud &    mean &     min &     max &   std \\
\midrule
     download data &   local &       aws &   20.58 &   17.23 &   31.80 &  2.77 \\
     download data &   local &     azure &   20.81 &   13.56 &   42.70 &  6.94 \\
     download data &   local &    docker &  820.98 &  820.98 &  820.98 &  0.00 \\
     download data &   local &    google &   18.00 &   17.06 &   19.38 &  0.48 \\
     download data &   local &    pi 3b+ &  130.17 &  123.84 &  149.40 &  5.39 \\
     download data &   local &      pi 4 &   47.67 &   43.43 &   75.60 &  5.72 \\
\midrule
           predict &   local &       aws &    0.03 &    0.02 &    0.05 &  0.00 \\
           predict &   local &     azure &    0.02 &    0.01 &    0.03 &  0.00 \\
           predict &   local &    docker &    0.03 &    0.03 &    0.03 &  0.00 \\
           predict &   local &    google &    0.03 &    0.01 &    0.06 &  0.00 \\
           predict &   local &  mac book &    0.12 &    0.12 &    0.12 &  0.00 \\
           predict &   local &    pi 3b+ &    0.12 &    0.10 &    0.14 &  0.01 \\
           predict &   local &      pi 4 &    0.08 &    0.08 &    0.08 &  0.00 \\
\midrule
           predict &  remote &       aws &    0.40 &    0.26 &    0.80 &  0.18 \\
           predict &  remote &     azure &    0.36 &    0.24 &    0.60 &  0.13 \\
           predict &  remote &    google &    0.36 &    0.27 &    0.82 &  0.16 \\
\midrule
 scikitlearn train &   local &       aws &   35.89 &   35.11 &   46.45 &  1.77 \\
 scikitlearn train &   local &     azure &   40.13 &   34.95 &   43.96 &  3.29 \\
 scikitlearn train &   local &    docker &   53.76 &   53.76 &   53.76 &  0.00 \\
 scikitlearn train &   local &    google &   42.13 &   41.77 &   42.49 &  0.13 \\
 scikitlearn train &   local &  mac book &   32.53 &   32.53 &   32.53 &  0.00 \\
 scikitlearn train &   local &    pi 3b+ &  222.63 &  209.18 &  231.90 &  7.87 \\
 scikitlearn train &   local &      pi 4 &   88.32 &   87.78 &   89.14 &  0.33 \\
\midrule
             train &   local &       aws &   35.72 &   34.91 &   46.50 &  1.73 \\
             train &   local &     azure &   40.28 &   35.30 &   47.50 &  3.32 \\
             train &   local &    docker &   54.72 &   54.72 &   54.72 &  0.00 \\
             train &   local &    google &   42.04 &   41.52 &   45.93 &  0.71 \\
             train &   local &  mac book &   33.82 &   33.82 &   33.82 &  0.00 \\
             train &   local &    pi 3b+ &  222.61 &  208.56 &  233.48 &  8.40 \\
             train &   local &      pi 4 &   88.59 &   87.83 &   89.35 &  0.32 \\
\midrule
            upload &   local &       aws &    0.01 &    0.01 &    0.01 &  0.00 \\
            upload &   local &     azure &    0.01 &    0.00 &    0.01 &  0.00 \\
            upload &   local &    docker &    0.02 &    0.02 &    0.02 &  0.00 \\
            upload &   local &    google &    0.01 &    0.01 &    0.01 &  0.00 \\
            upload &   local &  mac book &    0.02 &    0.02 &    0.02 &  0.00 \\
            upload &   local &    pi 3b+ &    0.09 &    0.04 &    0.48 &  0.08 \\
            upload &   local &      pi 4 &    0.02 &    0.02 &    0.02 &  0.00 \\
\midrule
            upload &  remote &       aws &    0.43 &    0.16 &    1.13 &  0.21 \\
            upload &  remote &     azure &    0.32 &    0.15 &    0.50 &  0.15 \\
            upload &  remote &    google &    0.31 &    0.18 &    0.73 &  0.18 \\
\bottomrule
\end{tabular}
}
\bigskip

\caption{Test results for the Eigenfaces SVM benchmark deployed
 as a multi-cloud service.}
\label{tab:3}

\resizebox{0.9\columnwidth}{!}{
\begin{tabular}{lllrrrr}
\toprule
test & type & cloud & mean & min & max & std \\
\midrule
download data & remote & aws    & 20.51 & 17.57 & 34.42 & 3.82 \\
download data & remote & azure  & 18.60 & 13.49 & 32.65 & 4.53 \\
download data & remote & google & 17.90 & 17.13 & 21.86 & 0.85 \\
\midrule
predict       & remote & aws	&  4.15 &  3.59 &  5.42 & 0.57 \\
predict       & remote & azure 	&  3.93 &  3.40 &  6.65 & 0.74 \\
predict       & remote & google &  4.13 &  3.74 &  6.37 & 0.60 \\
\midrule
train 	      & remote & aws 	& 35.61 & 35.24 & 39.53 & 0.73 \\
train 	      & remote & azure 	& 35.89 & 35.08 & 40.00 & 0.95 \\
train 	      & remote & google & 41.98 & 41.58 & 45.71 & 0.71 \\
\midrule
upload 	      & remote & aws 	& 10.08 &  4.89 & 16.52 & 4.38 \\
upload 	      & remote & azure 	&  8.46 &  4.72 & 13.92 & 4.05 \\
upload 	      & remote & google &  8.87 &  5.39 & 15.44 & 4.52 \\
\bottomrule
\end{tabular}
}


\end{table}

\begin{table}[htb]
\caption{Cost Analysis of function runtimes with \% cost increase and \% runtime decrease relative to the Raspberry Pi 3B+. }
\label{tab:cost}
\resizebox{1.0\columnwidth}{!}{
\begin{tabular}{lllrrrr}
\toprule
              test &    type &     cloud &    mean &     cost &  \% runtime decrease &  \% cost increase \\
\midrule
     download data &   local &       aws &   20.58 & 5.72e-04 &               NA &           NA \\
     download data &   local &     azure &   20.81 & 5.55e-04 &               NA &           NA \\
     download data &   local &    google &   18.00 & 4.75e-04 &               NA &           NA \\
\midrule
           predict &   local &       aws &    0.03 & 8.33e-07 &               75.00 &           281.87 \\
           predict &   local &     azure &    0.02 & 5.33e-07 &               83.33 &           144.39 \\
           predict &   local &    google &    0.03 & 7.92e-07 &               75.00 &           262.77 \\
           predict &   local &  mac book &    0.12 &      NA &                0.00 &              NA \\
           predict &   local &    docker &    0.03 &      NA &               75.00 &              NA \\
           predict &   local &      pi 4 &    0.08 & 2.96e-07 &               33.33 &            35.68 \\
           predict &   local &    pi 3b+ &    0.12 & 2.18e-07 &                0.00 &             0.00 \\
\midrule
           predict &  remote &       aws &    0.40 & 1.11e-05 &                 NA &              NA \\
           predict &  remote &     azure &    0.36 & 9.60e-06 &                 NA &              NA \\
           predict &  remote &    google &    0.36 & 9.50e-06 &                 NA &              NA \\
\midrule
 scikitlearn train &   local &       aws &   35.89 & 9.97e-04 &               83.88 &           146.24 \\
 scikitlearn train &   local &     azure &   40.13 & 1.07e-03 &               81.97 &           164.32 \\
 scikitlearn train &   local &    google &   42.13 & 1.11e-03 &               81.08 &           174.60 \\
 scikitlearn train &   local &  mac book &   32.53 &      NA &               85.39 &              NA \\
 scikitlearn train &   local &    docker &   53.76 &      NA &               75.85 &              NA \\
 scikitlearn train &   local &      pi 4 &   88.32 & 3.27e-04 &               60.33 &           -19.26 \\
 scikitlearn train &   local &    pi 3b+ &  222.63 & 4.05e-04 &                0.00 &             0.00 \\
\midrule
             train &   local &       aws &   35.72 & 9.92e-04 &               83.95 &           145.10 \\
             train &   local &     azure &   40.28 & 1.07e-03 &               81.91 &           165.33 \\
             train &   local &    google &   42.04 & 1.11e-03 &               81.11 &           174.04 \\
             train &   local &  mac book &   33.82 &      NA &               84.81 &              NA \\
             train &   local &    docker &   54.72 &      NA &               75.42 &              NA \\
             train &   local &      pi 4 &   88.59 & 3.28e-04 &               60.20 &           -19.01 \\
             train &   local &    pi 3b+ &  222.61 & 4.05e-04 &                0.00 &             0.00 \\
\midrule
            upload &   local &       aws &    0.01 & 2.78e-07 &               88.89 &            69.72 \\
            upload &   local &     azure &    0.01 & 2.67e-07 &               88.89 &            62.93 \\
            upload &   local &    google &    0.01 & 2.64e-07 &               88.89 &            61.23 \\
            upload &   local &  mac book &    0.02 &      NA &               77.78 &              NA \\
            upload &   local &    docker &    0.02 &      NA &               77.78 &              NA \\
            upload &   local &      pi 4 &    0.02 & 7.40e-08 &               77.78 &           -54.77 \\
            upload &   local &    pi 3b+ &    0.09 & 1.64e-07 &                0.00 &             0.00 \\
\midrule
            upload &  remote &       aws &    0.43 & 1.19e-05 &                 NA &              NA \\
            upload &  remote &     azure &    0.32 & 8.53e-06 &                 NA &              NA \\
            upload &  remote &    google &    0.31 & 8.18e-06 &                 NA &              NA \\
\bottomrule
\end{tabular}
}
\end{table}

\subsection{Multi-Cloud AI Service Benchmark}
\label{sec-multi-benchmark}

In this benchmark, our script first acquires VMs, installs Cloudmesh
OpenAPI, and launches the Eigenfaces SVM AI service on three separate
cloud providers. Because Cloudmesh has limited parallel computing
support, the script deploys the VMs in a serial manner. After the
services are running, we then run our tests in a parallel manner as
depicted in \Figure{fig:2}. Testing in parallel provides faster benchmark
results and better equalizes benchmark testing conditions. The
benchmark conducts requests to each cloud in parallel, so they experience similar network conditions. For example, in a serial testing
model when downloading data from a remote server, the remote server may experience varying loads which will ultimately result in different through puts for the various tests. Our parallel tests better equalize these conditions by having each cloud download the data under the same network conditions.

In the benchmark, we compute the means from 30 runs of a workflow that includes one download data invocation, one train invocation,
30 upload invocations, and 30 predict invocations. We run the workflows
in parallel on the separate clouds using multiprocessing on an eight-core
machine.

In \Figure{fig:7} we depict the combined runtime of our benchmark tests. This allows us to compare the complete execution time of an AI service workflow.

\OneFIGURE
    {ai_service_workflow_runtime.png}
    {Mean runtime of the Eigenfaces SVM workflow deployed
     as a multi-cloud service.}
    {fig:7}

In \Table{tab:3} we provide complete test results for the multi-cloud benchmark.

\begin{comment}
\subsubsection{Pipelined Anova SVM Example}
\label{pipelined-anova-svm-example}
\end{comment}

\begin{comment}
\subsection{Caleb Example}\label{caleb-example}
\end{comment}

\section{Limitations}\label{limitations}

Azure has updated their libraries and discontinued the version 4.0 Azure
libraries. We updated Cloudmesh to use the new library, but not all
features, such as virtual machine delete, are implemented or verified.

\section{Conclusion}
\label{sec:conclusion}
However, our most significant gain from this project is the reduction in manpower and entry barrier it takes to create and deploy our AI services. Due to the generalized approach while using python function developers and data scientists can naturally integrate more complex tasks as well as tasks that leverage cloud-specific AI services that are uniquely offered by particular providers. GAS Generator is an open-source project, and we appreciate contributions to the project. Please contact the first author at  \textit{laszewski\@gmail.com}.


\section*{Acknowledgment}
We would like to thank
Brian Kegerreis,
Jonathan Beckford,
Jagadeesh Kandimalla,
Prateek Shaw,
Ishan Mishra,
Fugang Wang, and Andrew Goldfarb for developing the Python REST service generator on which this work is based on. We also like to thank the more than 70 contributors to the cloudmesh toolkit for developing the various cloud providers. We would like to thank the NBDIF working group for their contributions in discussions that lead to the development of this effort.